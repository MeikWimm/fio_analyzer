@INPROCEEDINGS{9256518,
  author={Li, Yuegang and Ou, Dongyang and Jiang, Congfeng and Shen, Jing and Guo, Shuangshuang and Liu, Yin and Tang, Linlin},
  booktitle={2020 International Conference on Communications, Computing, Cybersecurity, and Informatics (CCCI)}, 
  title={Virtual Machine Performance Analysis and Prediction}, 
  year={2020},
  volume={},
  number={},
  pages={1-5},
  keywords={Virtual machining;Benchmark testing;Bandwidth;Companies;Cloud computing;Synchronization;Prediction algorithms;virtual machine performance;performance prediction;virtual machine consistency;performance analysis},
  doi={10.1109/CCCI49893.2020.9256518}
}
@INPROCEEDINGS{10.1145/2493123.2462919,
  author = {Marathe, Aniruddha and Harris, Rachel and Lowenthal, David K. and de Supinski, Bronis R. and Rountree, Barry and Schulz, Martin and Yuan, Xin},
  title = {A comparative study of high-performance computing on the cloud},
  year = {2013},
  isbn = {9781450319102},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2493123.2462919},
  doi = {10.1145/2493123.2462919},
  abstract = {The popularity of Amazon's EC2 cloud platform has increased in recent years. However, many high-performance computing (HPC) users consider dedicated high-performance clusters, typically found in large compute centers such as those in national laboratories, to be far superior to EC2 because of significant communication overhead of the latter. Our view is that this is quite narrow and the proper metrics for comparing high-performance clusters to EC2 is turnaround time and cost.In this paper, we compare the top-of-the-line EC2 cluster to HPC clusters at Lawrence Livermore National Laboratory (LLNL) based on turnaround time and total cost of execution. When measuring turnaround time, we include expected queue wait time on HPC clusters. Our results show that although as expected, standard HPC clusters are superior in raw performance, EC2 clusters may produce better turnaround times. To estimate cost, we developed a pricing model---relative to EC2's node-hour prices---to set node-hour prices for (currently free) LLNL clusters. We observe that the cost-effectiveness of running an application on a cluster depends on raw performance and application scalability.},
  booktitle = {Proceedings of the 22nd International Symposium on High-Performance Parallel and Distributed Computing},
  pages = {239–250},
  numpages = {12},
  keywords = {turnaround time, high-performance computing, cost, cloud},
  location = {New York, New York, USA},
  series = {HPDC '13}
}
@INPROCEEDINGS{whenStopPerformanceTest,
  author={Alghmadi, Hammam M. and Syer, Mark D. and Shang, Weiyi and Hassan, Ahmed E.},
  booktitle={2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={An Automated Approach for Recommending When to Stop Performance Tests}, 
  year={2016},
  volume={},
  number={},
  pages={279-289},
  keywords={Radiation detectors;Testing;System performance;Market research;Software systems;Software maintenance;Time factors;Software Engineering;Performance Testing;Performance Analysts;Performance Counters Data},
  doi={10.1109/ICSME.2016.46}
}
@ARTICLE{axboe2021github,
  title={GitHub—axboe/fio: Flexible I/O Tester},
  author={Axboe, Jens},
  journal={Retrieved Nov},
  volume={10},
  pages={2022},
  year={2021}
}
@ARTICLE{vmsHotandCold,
  author = {Barrett, Edd and Bolz-Tereick, Carl Friedrich and Killick, Rebecca and Mount, Sarah and Tratt, Laurence},
  title = {Virtual machine warmup blows hot and cold},
  year = {2017},
  issue_date = {October 2017},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {1},
  number = {OOPSLA},
  url = {https://doi.org/10.1145/3133876},
  doi = {10.1145/3133876},
  abstract = {Virtual Machines (VMs) with Just-In-Time (JIT) compilers are traditionally thought to execute programs in two phases: the initial warmup phase determines which parts of a program would most benefit from dynamic compilation, before JIT compiling those parts into machine code; subsequently the program is said to be at a steady state of peak performance. Measurement methodologies almost always discard data collected during the warmup phase such that reported measurements focus entirely on peak performance. We introduce a fully automated statistical approach, based on changepoint analysis, which allows us to determine if a program has reached a steady state and, if so, whether that represents peak performance or not. Using this, we show that even when run in the most controlled of circumstances, small, deterministic, widely studied microbenchmarks often fail to reach a steady state of peak performance on a variety of common VMs. Repeating our experiment on 3 different machines, we found that at most 43.5\% of <VM, Benchmark> pairs consistently reach a steady state of peak performance.},
  journal = {Proc. ACM Program. Lang.},
  month = oct,
  articleno = {52},
  numpages = {27},
  keywords = {performance, benchmarking, Virtual machine, JIT}
}
@INPROCEEDINGS{baseline,
  author={Reichelt, David Georg and Kühne, Stefan and Hasselbring, Wilhelm},
  booktitle={2022 IEEE 22nd International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={Automated Identification of Performance Changes at Code Level}, 
  year={2022},
  volume={},
  number={},
  pages={916-925},
  keywords={Codes;Software quality;Software performance;Benchmark testing;Reliability engineering;Software reliability;Software measurement;software performance engineering;performance measurement;benchmarking},
  doi={10.1109/QRS57517.2022.00096}
}
