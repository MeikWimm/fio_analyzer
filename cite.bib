@INPROCEEDINGS{9256518,
  author={Li, Yuegang and Ou, Dongyang and Jiang, Congfeng and Shen, Jing and Guo, Shuangshuang and Liu, Yin and Tang, Linlin},
  booktitle={2020 International Conference on Communications, Computing, Cybersecurity, and Informatics (CCCI)}, 
  title={Virtual Machine Performance Analysis and Prediction}, 
  year={2020},
  volume={},
  number={},
  pages={1-5},
  keywords={Virtual machining;Benchmark testing;Bandwidth;Companies;Cloud computing;Synchronization;Prediction algorithms;virtual machine performance;performance prediction;virtual machine consistency;performance analysis},
  doi={10.1109/CCCI49893.2020.9256518}
}
@INPROCEEDINGS{10.1145/2493123.2462919,
  author = {Marathe, Aniruddha and Harris, Rachel and Lowenthal, David K. and de Supinski, Bronis R. and Rountree, Barry and Schulz, Martin and Yuan, Xin},
  title = {A comparative study of high-performance computing on the cloud},
  year = {2013},
  isbn = {9781450319102},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2493123.2462919},
  doi = {10.1145/2493123.2462919},
  abstract = {The popularity of Amazon's EC2 cloud platform has increased in recent years. However, many high-performance computing (HPC) users consider dedicated high-performance clusters, typically found in large compute centers such as those in national laboratories, to be far superior to EC2 because of significant communication overhead of the latter. Our view is that this is quite narrow and the proper metrics for comparing high-performance clusters to EC2 is turnaround time and cost.In this paper, we compare the top-of-the-line EC2 cluster to HPC clusters at Lawrence Livermore National Laboratory (LLNL) based on turnaround time and total cost of execution. When measuring turnaround time, we include expected queue wait time on HPC clusters. Our results show that although as expected, standard HPC clusters are superior in raw performance, EC2 clusters may produce better turnaround times. To estimate cost, we developed a pricing model---relative to EC2's node-hour prices---to set node-hour prices for (currently free) LLNL clusters. We observe that the cost-effectiveness of running an application on a cluster depends on raw performance and application scalability.},
  booktitle = {Proceedings of the 22nd International Symposium on High-Performance Parallel and Distributed Computing},
  pages = {239–250},
  numpages = {12},
  keywords = {turnaround time, high-performance computing, cost, cloud},
  location = {New York, New York, USA},
  series = {HPDC '13}
}
@INPROCEEDINGS{whenStopPerformanceTest,
  author={Alghmadi, Hammam M. and Syer, Mark D. and Shang, Weiyi and Hassan, Ahmed E.},
  booktitle={2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={An Automated Approach for Recommending When to Stop Performance Tests}, 
  year={2016},
  volume={},
  number={},
  pages={279-289},
  keywords={Radiation detectors;Testing;System performance;Market research;Software systems;Software maintenance;Time factors;Software Engineering;Performance Testing;Performance Analysts;Performance Counters Data},
  doi={10.1109/ICSME.2016.46}
}
@ARTICLE{axboe2021github,
  title={GitHub—axboe/fio: Flexible I/O Tester},
  author={Axboe, Jens},
  journal={Retrieved Nov},
  volume={10},
  pages={2022},
  year={2021}
}
@ARTICLE{vmsHotandCold,
  author = {Barrett, Edd and Bolz-Tereick, Carl Friedrich and Killick, Rebecca and Mount, Sarah and Tratt, Laurence},
  title = {Virtual machine warmup blows hot and cold},
  year = {2017},
  issue_date = {October 2017},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {1},
  number = {OOPSLA},
  url = {https://doi.org/10.1145/3133876},
  doi = {10.1145/3133876},
  abstract = {Virtual Machines (VMs) with Just-In-Time (JIT) compilers are traditionally thought to execute programs in two phases: the initial warmup phase determines which parts of a program would most benefit from dynamic compilation, before JIT compiling those parts into machine code; subsequently the program is said to be at a steady state of peak performance. Measurement methodologies almost always discard data collected during the warmup phase such that reported measurements focus entirely on peak performance. We introduce a fully automated statistical approach, based on changepoint analysis, which allows us to determine if a program has reached a steady state and, if so, whether that represents peak performance or not. Using this, we show that even when run in the most controlled of circumstances, small, deterministic, widely studied microbenchmarks often fail to reach a steady state of peak performance on a variety of common VMs. Repeating our experiment on 3 different machines, we found that at most 43.5\% of <VM, Benchmark> pairs consistently reach a steady state of peak performance.},
  journal = {Proc. ACM Program. Lang.},
  month = oct,
  articleno = {52},
  numpages = {27},
  keywords = {performance, benchmarking, Virtual machine, JIT}
}
@INPROCEEDINGS{baseline,
  author={Reichelt, David Georg and Kühne, Stefan and Hasselbring, Wilhelm},
  booktitle={2022 IEEE 22nd International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={Automated Identification of Performance Changes at Code Level}, 
  year={2022},
  volume={},
  number={},
  pages={916-925},
  keywords={Codes;Software quality;Software performance;Benchmark testing;Reliability engineering;Software reliability;Software measurement;software performance engineering;performance measurement;benchmarking},
  doi={10.1109/QRS57517.2022.00096}
}
@inproceedings{10.1145/1297027.1297033,
author = {Georges, Andy and Buytaert, Dries and Eeckhout, Lieven},
title = {Statistically rigorous java performance evaluation},
year = {2007},
isbn = {9781595937865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1297027.1297033},
doi = {10.1145/1297027.1297033},
abstract = {Java performance is far from being trivial to benchmark because it is affected by various factors such as the Java application, its input, the virtual machine, the garbage collector, the heap size, etc. In addition, non-determinism at run-time causes the execution time of a Java program to differ from run to run. There are a number of sources of non-determinism such as Just-In-Time (JIT) compilation and optimization in the virtual machine (VM) driven by timer-based method sampling, thread scheduling, garbage collection, and various.There exist a wide variety of Java performance evaluation methodologies usedby researchers and benchmarkers. These methodologies differ from each other in a number of ways. Some report average performance over a number of runs of the same experiment; others report the best or second best performance observed; yet others report the worst. Some iterate the benchmark multiple times within a single VM invocation; others consider multiple VM invocations and iterate a single benchmark execution; yet others consider multiple VM invocations and iterate the benchmark multiple times.This paper shows that prevalent methodologies can be misleading, and can even lead to incorrect conclusions. The reason is that the data analysis is not statistically rigorous. In this paper, we present a survey of existing Java performance evaluation methodologies and discuss the importance of statistically rigorous data analysis for dealing with non-determinism. We advocate approaches to quantify startup as well as steady-state performance, and, in addition, we provide the JavaStats software to automatically obtain performance numbers in a rigorous manner. Although this paper focuses on Java performance evaluation, many of the issues addressed in this paper also apply to other programming languages and systems that build on a managed runtime system.},
booktitle = {Proceedings of the 22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications},
pages = {57–76},
numpages = {20},
keywords = {statistics, methodology, java, data analysis, benchmarking},
location = {Montreal, Quebec, Canada},
series = {OOPSLA '07}
}
@article{stasticsInPerformance,
author = {Georges, Andy and Buytaert, Dries and Eeckhout, Lieven},
title = {Statistically rigorous java performance evaluation},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/1297105.1297033},
doi = {10.1145/1297105.1297033},
abstract = {Java performance is far from being trivial to benchmark because it is affected by various factors such as the Java application, its input, the virtual machine, the garbage collector, the heap size, etc. In addition, non-determinism at run-time causes the execution time of a Java program to differ from run to run. There are a number of sources of non-determinism such as Just-In-Time (JIT) compilation and optimization in the virtual machine (VM) driven by timer-based method sampling, thread scheduling, garbage collection, and various.There exist a wide variety of Java performance evaluation methodologies usedby researchers and benchmarkers. These methodologies differ from each other in a number of ways. Some report average performance over a number of runs of the same experiment; others report the best or second best performance observed; yet others report the worst. Some iterate the benchmark multiple times within a single VM invocation; others consider multiple VM invocations and iterate a single benchmark execution; yet others consider multiple VM invocations and iterate the benchmark multiple times.This paper shows that prevalent methodologies can be misleading, and can even lead to incorrect conclusions. The reason is that the data analysis is not statistically rigorous. In this paper, we present a survey of existing Java performance evaluation methodologies and discuss the importance of statistically rigorous data analysis for dealing with non-determinism. We advocate approaches to quantify startup as well as steady-state performance, and, in addition, we provide the JavaStats software to automatically obtain performance numbers in a rigorous manner. Although this paper focuses on Java performance evaluation, many of the issues addressed in this paper also apply to other programming languages and systems that build on a managed runtime system.},
journal = {SIGPLAN Not.},
month = oct,
pages = {57–76},
numpages = {20},
keywords = {statistics, methodology, java, data analysis, benchmarking}
}
@inproceedings{analysisNVMeSSD,
author = {Xu, Qiumin and Siyamwala, Huzefa and Ghosh, Mrinmoy and Suri, Tameesh and Awasthi, Manu and Guz, Zvika and Shayesteh, Anahita and Balakrishnan, Vijay},
title = {Performance analysis of NVMe SSDs and their implication on real world databases},
year = {2015},
isbn = {9781450336079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2757667.2757684},
doi = {10.1145/2757667.2757684},
abstract = {The storage subsystem has undergone tremendous innovation in order to keep up with the ever-increasing demand for throughput. Non Volatile Memory Express (NVMe) based solid state devices are the latest development in this domain, delivering unprecedented performance in terms of latency and peak bandwidth. NVMe drives are expected to be particularly beneficial for I/O intensive applications, with databases being one of the prominent use-cases.This paper provides the first, in-depth performance analysis of NVMe drives. Combining driver instrumentation with system monitoring tools, we present a breakdown of access times for I/O requests throughout the entire system. Furthermore, we present a detailed, quantitative analysis of all the factors contributing to the low-latency, high-throughput characteristics of NVMe drives, including the system software stack. Lastly, we characterize the performance of multiple cloud databases (both relational and NoSQL) on state-of-the-art NVMe drives, and compare that to their performance on enterprise-class SATA-based SSDs. We show that NVMe-backed database applications deliver up to 8\texttimes{} superior client-side performance over enterprise-class, SATA-based SSDs.},
booktitle = {Proceedings of the 8th ACM International Systems and Storage Conference},
articleno = {6},
numpages = {11},
keywords = {performance characterization, hyperscale applications, SSD, NoSQL databases, NVMe},
location = {Haifa, Israel},
series = {SYSTOR '15}
}
@Inbook{benchmarkingBruno2014,
author="Bruno, Isabelle",
editor="Michalos, Alex C.",
title="Benchmarking",
bookTitle="Encyclopedia of Quality of Life and Well-Being Research",
year="2014",
publisher="Springer Netherlands",
address="Dordrecht",
pages="363--368",
isbn="978-94-007-0753-5",
doi="10.1007/978-94-007-0753-5_170",
url="https://doi.org/10.1007/978-94-007-0753-5_170"
}






