% This file was created with JabRef 2.10b2.
% Encoding: UTF-8


@Book{DBLP:books/sp/HarderR01,
  Title                    = {{Datenbanksysteme: Konzepte und Techniken der Implementierung}},
  Author                   = {Theo Härder and Erhard Rahm},
  Publisher                = {Springer},
  Year                     = {2001},

  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  ISBN                     = {3-540-42133-5}
}

@INPROCEEDINGS{statistics,
  author={Chen, Tianshi and Chen, Yunji and Guo, Qi and Temam, Olivier and Wu, Yue and Hu, Weiwu},
  booktitle={IEEE International Symposium on High-Performance Comp Architecture}, 
  title={Statistical performance comparisons of computers}, 
  year={2012},
  volume={},
  number={},
  pages={1-12},
  keywords={Computers;Benchmark testing;Gaussian distribution;Size measurement;Data structures},
  doi={10.1109/HPCA.2012.6169043}
}

@book{Lilja_2000, place={Cambridge}, 
  title={Measuring Computer Performance: A Practitioner’s Guide}, 
  publisher={Cambridge University Press}, 
  author={Lilja, David J.}, 
  year={2000}}
@article{tukey_HSD,
  author = {Nanda, Anita and Mahapatra, Abikesh and Mohapatra, Bibhuti and mahapatra, abinash},
  year = {2021},
  month = {01},
  pages = {59-65},
  title = {Multiple comparison test by Tukey's honestly significant difference (HSD): Do the confident level control type I error},
  volume = {6},
  journal = {International Journal of Applied Mathematics and Statistics},
  doi = {10.22271/maths.2021.v6.i1a.636}
}
@article{Vidgen_2016,
   title={P-Values: Misunderstood and Misused},
   volume={4},
   ISSN={2296-424X},
   url={http://dx.doi.org/10.3389/fphy.2016.00006},
   DOI={10.3389/fphy.2016.00006},
   journal={Frontiers in Physics},
   publisher={Frontiers Media SA},
   author={Vidgen, Bertie and Yasseri, Taha},
   year={2016},
   month=mar 
  }

  @book{book,
author = {Howell, David},
year = {2010},
pages = {414},
title = {Statistical Methods For Psychology},
volume = {43},
journal = {The Statistician},
doi = {10.2307/2348956}
}

@misc{benchmark_defi,
  title        = {CiteDrive brings reference management to Overleaf},
  author       = {CiteDrive, Inc},
  year         = 2022,
  note         = {\url{https://www.sciencedirect.com/topics/computer-science/performance-benchmark} [Accessed: (07.02.2025]}
}

@misc{axboe,
  title        = {fio-axboe},
  author       = {CiteDrive, Inc},
  year         = 2022,
  note         = {\url{https://github.com/axboe/fio}}
}

@article{nine-year-of-bench,
author = {Traeger, Avishay and Zadok, Erez and Joukov, Nikolai and Wright, Charles P.},
title = {A nine year study of file system and storage benchmarking},
year = {2008},
issue_date = {May 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/1367829.1367831},
doi = {10.1145/1367829.1367831},
abstract = {Benchmarking is critical when evaluating performance, but is especially difficult for file and storage systems. Complex interactions between I/O devices, caches, kernel daemons, and other OS components result in behavior that is rather difficult to analyze. Moreover, systems have different features and optimizations, so no single benchmark is always suitable. The large variety of workloads that these systems experience in the real world also adds to this difficulty.In this article we survey 415 file system and storage benchmarks from 106 recent papers. We found that most popular benchmarks are flawed and many research papers do not provide a clear indication of true performance. We provide guidelines that we hope will improve future performance evaluations. To show how some widely used benchmarks can conceal or overemphasize overheads, we conducted a set of experiments. As a specific example, slowing down read operations on ext2 by a factor of 32 resulted in only a 2--5\% wall-clock slowdown in a popular compile benchmark. Finally, we discuss future work to improve file system and storage benchmarking.},
journal = {ACM Trans. Storage},
month = may,
articleno = {5},
numpages = {56},
keywords = {Benchmarks, file systems, storage systems}
}

@article{benchmark_defi_for_intro,
  title={Benchmarking computers and computer networks},
  author={Bouckaert, Stefan and Gerwen, J and Moerman, Ingrid and Phillips, Stephen C and Wilander, Jerker and Rehman, Shafqat Ur and Dabbous, Walid and Turletti, Thierry},
  journal={EU FIRE White Paper},
  year={2010}
}

@article{transient_state_definition,
author = {Moxnes, Erling and Davidsen, Pål},
year = {2016},
month = {04},
pages = {128-153},
title = {Intuitive understanding of steady-state and transient behaviors: Steady-state and transients},
volume = {32},
journal = {System Dynamics Review},
doi = {10.1002/sdr.1561}
}
@article{statistically_rigorous,
author = {Georges, Andy and Buytaert, Dries and Eeckhout, Lieven},
title = {Statistically rigorous java performance evaluation},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/1297105.1297033},
doi = {10.1145/1297105.1297033},
abstract = {Java performance is far from being trivial to benchmark because it is affected by various factors such as the Java application, its input, the virtual machine, the garbage collector, the heap size, etc. In addition, non-determinism at run-time causes the execution time of a Java program to differ from run to run. There are a number of sources of non-determinism such as Just-In-Time (JIT) compilation and optimization in the virtual machine (VM) driven by timer-based method sampling, thread scheduling, garbage collection, and various.There exist a wide variety of Java performance evaluation methodologies usedby researchers and benchmarkers. These methodologies differ from each other in a number of ways. Some report average performance over a number of runs of the same experiment; others report the best or second best performance observed; yet others report the worst. Some iterate the benchmark multiple times within a single VM invocation; others consider multiple VM invocations and iterate a single benchmark execution; yet others consider multiple VM invocations and iterate the benchmark multiple times.This paper shows that prevalent methodologies can be misleading, and can even lead to incorrect conclusions. The reason is that the data analysis is not statistically rigorous. In this paper, we present a survey of existing Java performance evaluation methodologies and discuss the importance of statistically rigorous data analysis for dealing with non-determinism. We advocate approaches to quantify startup as well as steady-state performance, and, in addition, we provide the JavaStats software to automatically obtain performance numbers in a rigorous manner. Although this paper focuses on Java performance evaluation, many of the issues addressed in this paper also apply to other programming languages and systems that build on a managed runtime system.},
journal = {SIGPLAN Not.},
month = oct,
pages = {57–76},
numpages = {20},
keywords = {benchmarking, data analysis, java, methodology, statistics}
}

@book{inferenzstatistik,
author = {Janczyk, Markus and Pfister, Roland},
address = {Berlin, Heidelberg},
copyright = {Springer-Verlag GmbH Deutschland, ein Teil von Springer Nature 2020},
edition = {3. Aufl. 2020},
isbn = {3662599082},
keywords = {Biostatistics ; Mathematics and Statistics ; Statistics ; Statistics for Life Sciences Medicine Health Sciences},
language = {ger},
publisher = {Springer Berlin Heidelberg},
title = {Inferenzstatistik verstehen: Von A wie Signifikanztest bis Z wie Konfidenzintervall},
year = {2020},
}
@book{kurt2020stochastik,
  title={Stochastik f{\"u}r Informatiker: Eine Einf{\"u}hrung in einheitlich strukturierten Lerneinheiten},
  author={Kurt, N.},
  isbn={9783662605158},
  url={https://books.google.de/books?id=Xn3ayQEACAAJ},
  year={2020},
  publisher={Springer Berlin Heidelberg}
}
@book{statistik_sozialwissenschaften,
  author={Bortz, J{\"u}rgen and Schuster, Christof},
  title={Statistik f{\"u}r Human- und Sozialwissenschaftler},
  edition={7., vollst{\"a}ndig {\"u}berarbeitete und erweiterte Auflage},
  publisher={Springer},
  address={Berlin},
  year={2010},
  pages={1 Online-Ressource (XX, 655 S. 70 Abb, digital)},
  language={ger},
  isbn={978-3-642-12770-0},
  series={SpringerLink : B{\"u}cher},
  doi={10.1007/978-3-642-12770-0},
  keywords={Sozialwissenschaften / Statistik ; Humanwissenschaften / Statistik},
  url={http://dx.doi.org/10.1007/978-3-642-12770-0},
  library={UB ; UW},
}

@INPROCEEDINGS{u_test,
  author={Chen, Tianshi and Chen, Yunji and Guo, Qi and Temam, Olivier and Wu, Yue and Hu, Weiwu},
  booktitle={IEEE International Symposium on High-Performance Comp Architecture}, 
  title={Statistical performance comparisons of computers}, 
  year={2012},
  volume={},
  number={},
  pages={1-12},
  keywords={Computers;Benchmark testing;Gaussian distribution;Size measurement;Data structures},
  doi={10.1109/HPCA.2012.6169043}}
