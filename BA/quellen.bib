% This file was created with JabRef 2.10b2.
% Encoding: UTF-8


@Book{DBLP:books/sp/HarderR01,
  Title                    = {{Datenbanksysteme: Konzepte und Techniken der Implementierung}},
  Author                   = {Theo Härder and Erhard Rahm},
  Publisher                = {Springer},
  Year                     = {2001},

  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  ISBN                     = {3-540-42133-5}
}

@INPROCEEDINGS{statistics,
  author={Chen, Tianshi and Chen, Yunji and Guo, Qi and Temam, Olivier and Wu, Yue and Hu, Weiwu},
  booktitle={IEEE International Symposium on High-Performance Comp Architecture}, 
  title={Statistical performance comparisons of computers}, 
  year={2012},
  volume={},
  number={},
  pages={1-12},
  keywords={Computers;Benchmark testing;Gaussian distribution;Size measurement;Data structures},
  doi={10.1109/HPCA.2012.6169043}
}

@book{Lilja_2000, place={Cambridge}, 
  title={Measuring Computer Performance: A Practitioner’s Guide}, 
  publisher={Cambridge University Press}, 
  author={Lilja, David J.}, 
  year={2000}}
@article{tukey_HSD,
  author = {Nanda, Anita and Mahapatra, Abikesh and Mohapatra, Bibhuti and mahapatra, abinash},
  year = {2021},
  month = {01},
  pages = {59-65},
  title = {Multiple comparison test by Tukey's honestly significant difference (HSD): Do the confident level control type I error},
  volume = {6},
  journal = {International Journal of Applied Mathematics and Statistics},
  doi = {10.22271/maths.2021.v6.i1a.636}
}
@article{Vidgen_2016,
   title={P-Values: Misunderstood and Misused},
   volume={4},
   ISSN={2296-424X},
   url={http://dx.doi.org/10.3389/fphy.2016.00006},
   DOI={10.3389/fphy.2016.00006},
   journal={Frontiers in Physics},
   publisher={Frontiers Media SA},
   author={Vidgen, Bertie and Yasseri, Taha},
   year={2016},
   month=mar 
  }

  @book{book,
author = {Howell, David},
year = {2010},
pages = {414},
title = {Statistical Methods For Psychology},
volume = {43},
journal = {The Statistician},
doi = {10.2307/2348956}
}

@misc{benchmark_defi,
  title        = {CiteDrive brings reference management to Overleaf},
  author       = {CiteDrive, Inc},
  year         = 2022,
  note         = {\url{https://www.sciencedirect.com/topics/computer-science/performance-benchmark} [Accessed: (07.02.2025]}
}

@misc{axboe,
  title        = {fio-axboe},
  author       = {CiteDrive, Inc},
  year         = 2022,
  note         = {\url{https://github.com/axboe/fio}}
}

@article{nine-year-of-bench,
author = {Traeger, Avishay and Zadok, Erez and Joukov, Nikolai and Wright, Charles P.},
title = {A nine year study of file system and storage benchmarking},
year = {2008},
issue_date = {May 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/1367829.1367831},
doi = {10.1145/1367829.1367831},
abstract = {Benchmarking is critical when evaluating performance, but is especially difficult for file and storage systems. Complex interactions between I/O devices, caches, kernel daemons, and other OS components result in behavior that is rather difficult to analyze. Moreover, systems have different features and optimizations, so no single benchmark is always suitable. The large variety of workloads that these systems experience in the real world also adds to this difficulty.In this article we survey 415 file system and storage benchmarks from 106 recent papers. We found that most popular benchmarks are flawed and many research papers do not provide a clear indication of true performance. We provide guidelines that we hope will improve future performance evaluations. To show how some widely used benchmarks can conceal or overemphasize overheads, we conducted a set of experiments. As a specific example, slowing down read operations on ext2 by a factor of 32 resulted in only a 2--5\% wall-clock slowdown in a popular compile benchmark. Finally, we discuss future work to improve file system and storage benchmarking.},
journal = {ACM Trans. Storage},
month = may,
articleno = {5},
numpages = {56},
keywords = {Benchmarks, file systems, storage systems}
}

@article{benchmark_defi_for_intro,
  title={Benchmarking computers and computer networks},
  author={Bouckaert, Stefan and Gerwen, J and Moerman, Ingrid and Phillips, Stephen C and Wilander, Jerker and Rehman, Shafqat Ur and Dabbous, Walid and Turletti, Thierry},
  journal={EU FIRE White Paper},
  year={2010}
}

@article{transient_state_definition,
author = {Moxnes, Erling and Davidsen, Pål},
year = {2016},
month = {04},
pages = {128-153},
title = {Intuitive understanding of steady-state and transient behaviors: Steady-state and transients},
volume = {32},
journal = {System Dynamics Review},
doi = {10.1002/sdr.1561}
}
@article{statistically_rigorous,
author = {Georges, Andy and Buytaert, Dries and Eeckhout, Lieven},
title = {Statistically rigorous java performance evaluation},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/1297105.1297033},
doi = {10.1145/1297105.1297033},
abstract = {Java performance is far from being trivial to benchmark because it is affected by various factors such as the Java application, its input, the virtual machine, the garbage collector, the heap size, etc. In addition, non-determinism at run-time causes the execution time of a Java program to differ from run to run. There are a number of sources of non-determinism such as Just-In-Time (JIT) compilation and optimization in the virtual machine (VM) driven by timer-based method sampling, thread scheduling, garbage collection, and various.There exist a wide variety of Java performance evaluation methodologies usedby researchers and benchmarkers. These methodologies differ from each other in a number of ways. Some report average performance over a number of runs of the same experiment; others report the best or second best performance observed; yet others report the worst. Some iterate the benchmark multiple times within a single VM invocation; others consider multiple VM invocations and iterate a single benchmark execution; yet others consider multiple VM invocations and iterate the benchmark multiple times.This paper shows that prevalent methodologies can be misleading, and can even lead to incorrect conclusions. The reason is that the data analysis is not statistically rigorous. In this paper, we present a survey of existing Java performance evaluation methodologies and discuss the importance of statistically rigorous data analysis for dealing with non-determinism. We advocate approaches to quantify startup as well as steady-state performance, and, in addition, we provide the JavaStats software to automatically obtain performance numbers in a rigorous manner. Although this paper focuses on Java performance evaluation, many of the issues addressed in this paper also apply to other programming languages and systems that build on a managed runtime system.},
journal = {SIGPLAN Not.},
month = oct,
pages = {57–76},
numpages = {20},
keywords = {benchmarking, data analysis, java, methodology, statistics}
}

@book{inferenzstatistik,
author = {Janczyk, Markus and Pfister, Roland},
address = {Berlin, Heidelberg},
copyright = {Springer-Verlag GmbH Deutschland, ein Teil von Springer Nature 2020},
edition = {3. Aufl. 2020},
isbn = {3662599082},
keywords = {Biostatistics ; Mathematics and Statistics ; Statistics ; Statistics for Life Sciences Medicine Health Sciences},
language = {ger},
publisher = {Springer Berlin Heidelberg},
title = {Inferenzstatistik verstehen: Von A wie Signifikanztest bis Z wie Konfidenzintervall},
year = {2020},
}
@book{kurt2020stochastik,
  title={Stochastik f{\"u}r Informatiker: Eine Einf{\"u}hrung in einheitlich strukturierten Lerneinheiten},
  author={Kurt, N.},
  isbn={9783662605158},
  url={https://books.google.de/books?id=Xn3ayQEACAAJ},
  year={2020},
  publisher={Springer Berlin Heidelberg}
}
@book{statistik_sozialwissenschaften,
  author={Bortz, J{\"u}rgen and Schuster, Christof},
  title={Statistik f{\"u}r Human- und Sozialwissenschaftler},
  edition={7., vollst{\"a}ndig {\"u}berarbeitete und erweiterte Auflage},
  publisher={Springer},
  address={Berlin},
  year={2010},
  pages={1 Online-Ressource (XX, 655 S. 70 Abb, digital)},
  language={ger},
  isbn={978-3-642-12770-0},
  series={SpringerLink : B{\"u}cher},
  doi={10.1007/978-3-642-12770-0},
  keywords={Sozialwissenschaften / Statistik ; Humanwissenschaften / Statistik},
  url={http://dx.doi.org/10.1007/978-3-642-12770-0},
  library={UB ; UW},
}

@INPROCEEDINGS{u_test,
  author={Chen, Tianshi and Chen, Yunji and Guo, Qi and Temam, Olivier and Wu, Yue and Hu, Weiwu},
  booktitle={IEEE International Symposium on High-Performance Comp Architecture}, 
  title={Statistical performance comparisons of computers}, 
  year={2012},
  volume={},
  number={},
  pages={1-12},
  keywords={Computers;Benchmark testing;Gaussian distribution;Size measurement;Data structures},
  doi={10.1109/HPCA.2012.6169043}}

@inproceedings{when_stop_tests,
author = {Alghmadi, Hammam and Syer, Mark and Shang, Weiyi and Hassan, Ahmed E.},
year = {2016},
month = {10},
pages = {279-289},
title = {An Automated Approach for Recommending When to Stop Performance Tests},
doi = {10.1109/ICSME.2016.46}
}

@article{defi_benchmarking,
  title={Measuring performance metrics: Techniques and tools},
  author={Sabetta, Antonino and Koziolek, Heiko},
  journal={Dependability Metrics: Advanced Lectures},
  pages={226--232},
  year={2008},
  publisher={Springer}
}

@inproceedings{ssd_benchmark,
author = {Xu, Qiumin and Siyamwala, Huzefa and Ghosh, Mrinmoy and Suri, Tameesh and Awasthi, Manu and Guz, Zvika and Shayesteh, Anahita and Balakrishnan, Vijay},
title = {Performance analysis of NVMe SSDs and their implication on real world databases},
year = {2015},
isbn = {9781450336079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2757667.2757684},
doi = {10.1145/2757667.2757684},
abstract = {The storage subsystem has undergone tremendous innovation in order to keep up with the ever-increasing demand for throughput. Non Volatile Memory Express (NVMe) based solid state devices are the latest development in this domain, delivering unprecedented performance in terms of latency and peak bandwidth. NVMe drives are expected to be particularly beneficial for I/O intensive applications, with databases being one of the prominent use-cases.This paper provides the first, in-depth performance analysis of NVMe drives. Combining driver instrumentation with system monitoring tools, we present a breakdown of access times for I/O requests throughout the entire system. Furthermore, we present a detailed, quantitative analysis of all the factors contributing to the low-latency, high-throughput characteristics of NVMe drives, including the system software stack. Lastly, we characterize the performance of multiple cloud databases (both relational and NoSQL) on state-of-the-art NVMe drives, and compare that to their performance on enterprise-class SATA-based SSDs. We show that NVMe-backed database applications deliver up to 8\texttimes{} superior client-side performance over enterprise-class, SATA-based SSDs.},
booktitle = {Proceedings of the 8th ACM International Systems and Storage Conference},
articleno = {6},
numpages = {11},
keywords = {performance characterization, hyperscale applications, SSD, NoSQL databases, NVMe},
location = {Haifa, Israel},
series = {SYSTOR '15}
}

@misc{wang2019benchmarkingtpugpucpu,
      title={Benchmarking TPU, GPU, and CPU Platforms for Deep Learning}, 
      author={Yu Emma Wang and Gu-Yeon Wei and David Brooks},
      year={2019},
      eprint={1907.10701},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1907.10701}, 
}

@article{warmAndCold,
author = {Barrett, Edd and Bolz-Tereick, Carl Friedrich and Killick, Rebecca and Mount, Sarah and Tratt, Laurence},
title = {Virtual machine warmup blows hot and cold},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {OOPSLA},
url = {https://doi.org/10.1145/3133876},
doi = {10.1145/3133876},
abstract = {Virtual Machines (VMs) with Just-In-Time (JIT) compilers are traditionally thought to execute programs in two phases: the initial warmup phase determines which parts of a program would most benefit from dynamic compilation, before JIT compiling those parts into machine code; subsequently the program is said to be at a steady state of peak performance. Measurement methodologies almost always discard data collected during the warmup phase such that reported measurements focus entirely on peak performance. We introduce a fully automated statistical approach, based on changepoint analysis, which allows us to determine if a program has reached a steady state and, if so, whether that represents peak performance or not. Using this, we show that even when run in the most controlled of circumstances, small, deterministic, widely studied microbenchmarks often fail to reach a steady state of peak performance on a variety of common VMs. Repeating our experiment on 3 different machines, we found that at most 43.5\% of <VM, Benchmark> pairs consistently reach a steady state of peak performance.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {52},
numpages = {27},
keywords = {JIT, Virtual machine, benchmarking, performance}
}

@inproceedings{baseline_paper,
   title={Automated Identification of Performance Changes at Code Level},
   url={http://dx.doi.org/10.1109/QRS57517.2022.00096},
   DOI={10.1109/qrs57517.2022.00096},
   booktitle={2022 IEEE 22nd International Conference on Software Quality, Reliability and Security (QRS)},
   publisher={IEEE},
   author={Reichelt, David Georg and Kühne, Stefan and Hasselbring, Wilhelm},
   year={2022},
   month=dec, pages={916–925} 
   }

   @INPROCEEDINGS{statistical_performance_pc,
  author={Chen, Tianshi and Chen, Yunji and Guo, Qi and Temam, Olivier and Wu, Yue and Hu, Weiwu},
  booktitle={IEEE International Symposium on High-Performance Comp Architecture}, 
  title={Statistical performance comparisons of computers}, 
  year={2012},
  volume={},
  number={},
  pages={1-12},
  keywords={Computers;Benchmark testing;Gaussian distribution;Size measurement;Data structures},
  doi={10.1109/HPCA.2012.6169043}}

