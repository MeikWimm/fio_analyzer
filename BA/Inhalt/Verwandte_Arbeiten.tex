\chapter{Verwandte Arbeiten}
\label{cha:Verwandte_Arbeiten}

Im Paper von Georges et al. \cite{statistically_rigorous} wurden mehrere Benchmark-Tools verwendet.
Dies wird auch von Traeger et. al \cite{nine-year-of-bench} empfohlen um faire Tests und vergleiche zu gewährleisten.
Xu et al. \cite{ssd_benchmark} führten Festplatten-Benchmarking mit dem Tool fio durch. 
Dabei wurde auf die I/O-Engine geachtet und eine asynchrone I/O-Engine genutzt genannt \textit{libaio},
um den Page-Cache zu umgehen und die Rohleistung der Festplatten zu analysieren.
Dabei wird ein Flag für das fio gesetzt.
Hierfür wurde das direct-Flag von fio aktiviert, 
welches in der dieser Arbeit unverändert übernommen wurde.
Zusätzlich nutzten Xu et al. \cite{ssd_benchmark} das Tool Blktrace 
(entwickelt von Axboe und Brunelle, 2007) für detaillierte Leistungsanalysen.
 Für die Bewertung der Performanz wurden zwei weitere Metriken herangezogen: 
die Submission Latency (slat) und die Completion Latency (clat).
Für die Evaluierung wurden verschiedene Datenbanken verwendet um realitätsnahe Tests zu ermöglichen.
Datenbankmanagementsysteme (DBMS) stellen hierbei ein typisches Beispiel für I/O-intensive Anwendungen in Rechenzentren dar.
Die Tests umfassten den Vergleich von SATA-HDDs, SATA-SSDs und 
NVMe-Laufwerken unter Berücksichtigung der beiden genannten Latenzmetriken.

Das Paper von Barrett et al. \cite{warmAndCold} beschäftigt sich zwar nicht mit Festplattenbenchmarking, thematisiert jedoch das Benchmarking sowie den stationären Zustand von virtuellen Maschinen (VMs).
Dabei ist mit Virtuelle Maschine hier eine JIT\footnote{Just-In-Time}-Kompilierung gemeint.
Auch Reichelt et al. \cite{baseline_paper} verwenden solche virtuellen Maschinen, um den stationären Zustand zu bestimmen. Dabei kommen statistische Tests wie der T-Test und der Mann-Whitney-Test zum Einsatz, 
ergänzt durch Vergleiche von Konfidenzintervallen.
In diesem Zusammenhang wird die zentrale Frage untersucht, wann der sogenannte "steady state of peak performance" innerhalb einer JIT-Kompilierung erreicht wird.
Die im Paper beschriebenen Methoden orientieren sich ebenfalls an den von Georges et al. \cite{statistically_rigorous} empfohlenen Vorgehensweisen.

Im Paper von Chen et al. \cite{statistical_performance_pc} wird ein Szenario vorgestellt, in dem keine Normalverteilung vorliegt.
Dabei kommen statistische Methoden wie das Normality Fitting (NNF) und das Kernel Parzen Window (KPW) zum Einsatz.
Zur Überprüfung der Normalverteilung der Leistung wird empirisch untersucht, 
ob die Wahrscheinlichkeitsdichtefunktion, die auf der Annahme einer Normalverteilung der Ausführungszeit basiert, 
mit der tatsächlichen Wahrscheinlichkeitsdichtefunktion der Ausführungszeit übereinstimmt.
Doch in Georges et. al \cite{statistically_rigorous} im Kontext der Leistungsbewertung von Java Virtual Machine (JVM) stellt fest, dass die Leistung einzelner Benchmarks (SPECjvm98) 
auf mehreren Single-Core-Computern im Allgemeinen durch Normalverteilungen charakterisiert werden kann.
Darüber hinaus wird in Chen et al. \cite{statistical_performance_pc} der Grenzwertsatz untersucht, 
um zu bestimmen, welche Anzahl von Messwerten erforderlich ist, damit der zentrale Grenzwertsatz erfüllt ist.

Im Paper von Alghmadi et al. \cite{when_stop_tests} wird untersucht, wann der Abbruch von Tests ausreichend ist. 
Zur Bestimmung des geeigneten Zeitpunkts für das Beenden von Messungen verwenden Alghmadi et al. 
\cite{when_stop_tests} den Mann-Whitney-Rang-Test. Dieser wird eingesetzt, um festzustellen, wann solche Tests beendet werden können. 
Ziel ist es, durch die Analyse repetitiver Muster in der Performance diese Zeitpunkte zu identifizieren und die Testdauer effizient zu reduzieren.




